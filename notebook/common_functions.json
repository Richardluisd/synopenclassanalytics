{
	"name": "common_functions",
	"properties": {
		"folder": {
			"name": "includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0e389758-dadd-42fc-88a4-91ad5befe012"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## common_functions\n",
					"\n",
					"undefined\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Importación de librerías necesarias**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.functions import substring, isnull, col, regexp_replace, current_timestamp, lit, month, dayofmonth, year, to_date, lpad, input_file_name, coalesce \n",
					"import pyspark.sql.functions as F\n",
					"from notebookutils import mssparkutils\n",
					"from delta import *\n",
					"from datetime import datetime, timedelta, date"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Definición de funciones**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# init last periodload delta table\n",
					"def initlastperiodload(df):\n",
					"    df.write.format(\"delta\").mode(\"append\").option(\"escape\",\"\\\"\").saveAsTable(\"log.lastperiodload\")\n",
					"\n",
					"# create lastperiodload delta table\n",
					"def createlastperiodload(path):\n",
					"    createDatabase(\"log\")\n",
					"    spark.sql(f\"         CREATE OR REPLACE TABLE log.lastperiodload (            name STRING,             lastperiodload TIMESTAMP         )           USING DELTA         LOCATION \\\"{path}\\\"     \")\n",
					"\n",
					"# update lastperiodload\n",
					"def updatelastperiodload(entity,lastperiodload):\n",
					"    spark.sql(f\"         UPDATE log.lastperiodload SET lastperiodload = \\\"{lastperiodload}\\\" where name = \\\"{entity}\\\"     \")\n",
					"\n",
					"# get lastperiodload\n",
					"def getlastperiodload(entity):\n",
					"    return spark.sql(f\"         SELECT name, lastperiodload from log.lastperiodload where name = \\\"{entity}\\\"     \").take(1)[0].lastperiodload\n",
					"\n",
					"# Generate date range\n",
					"def daterange(start_date=None, end_date=datetime.today(), default_range_date=30):\n",
					"    dates = []\n",
					"    if not start_date:\n",
					"        start_date = datetime.today() + timedelta(-default_range_date)\n",
					"    for n in range(int((end_date - start_date).days - 1)):\n",
					"        dates.append((start_date + timedelta(n+1)).strftime('%Y/%m/%d'))\n",
					"    return dates\n",
					"\n",
					"# Check is is date and Add year, month and day in dataframe from date\n",
					"def addDatesFields(df, date_column, level):\n",
					"    if level == \"day\":\n",
					"        try:\n",
					"            print(f\"addDatesFields: day - date_column: {date_column}\")\n",
					"            df = df.withColumn(f\"{date_column}_year\", coalesce(year(to_date(col(date_column),\"yyyy-MM-dd\")),lit(1900)))             .withColumn(f\"{date_column}_month\", coalesce(lpad(month(to_date(col(date_column),\"yyyy-MM-dd\")),2,\"0\"),lit(\"01\")))             .withColumn(f\"{date_column}_day\", coalesce(lpad(dayofmonth(to_date(col(date_column),\"yyyy-MM-dd\")),2,\"0\"),lit(\"01\")))\n",
					"            return df\n",
					"        except Exception as e:\n",
					"            print(f\"addDatesFields: day - exception: {e} - date_column: {date_column}\")\n",
					"            return df\n",
					"    if level == \"month\":\n",
					"        try:\n",
					"            print(f\"addDatesFields: month - date_column: {date_column}\")\n",
					"            df = df.withColumn(f\"{date_column}_year\", coalesce(year(to_date(col(date_column),\"yyyy-MM-dd\")),lit(1900)))             .withColumn(f\"{date_column}_month\", coalesce(lpad(month(to_date(col(date_column),\"yyyy-MM-dd\")),2,\"0\"),lit(\"01\")))\n",
					"            return df\n",
					"        except Exception as e:\n",
					"            print(f\"addDatesFields: month - exception: {e} - date_column: {date_column}\")\n",
					"            return df\n",
					"    if level == \"year\":\n",
					"        try:\n",
					"            print(f\"addDatesFields: year - date_column: {date_column}\")\n",
					"            df = df.withColumn(f\"{date_column}_year\", coalesce(year(to_date(col(date_column),\"yyyy-MM-dd\")),lit(1900)))\n",
					"            return df\n",
					"        except Exception as e:\n",
					"            print(f\"addDatesFields: year - exception: {e} - date_column: {date_column}\")\n",
					"            return df\n",
					"\n",
					"# Generate list patition field.\n",
					"def generatePartitionField(partition_column, level):\n",
					"    if level == \"date\":\n",
					"        return [f\"{partition_column}_year\",f\"{partition_column}_month\",f\"{partition_column}_day\"]\n",
					"    if level == \"month\":\n",
					"        return [f\"{partition_column}_year\",f\"{partition_column}_month\"]\n",
					"    if level == \"year\":\n",
					"        return [f\"{partition_column}_year\"]\n",
					"\n",
					"# Clean characteres\n",
					"def cleanSpecialChar(df, char=\".@-\"):\n",
					"    print (\"clean special chars\")\n",
					"    return df.select([F.regexp_replace(col,f\"[^\\s\\w\\{char}]\",\"\").alias(col) for col in df.columns])\n",
					"\n",
					"def checkDeltaFolder(path):\n",
					"    try:\n",
					"        DeltaTable.forPath(spark, path)\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        return False\n",
					"\n",
					"# Verifica si existe una tabla\n",
					"def checkTableExist(database, table):\n",
					"    return spark._jsparkSession.catalog().tableExists(f\"{database}.{table}\")\n",
					"\n",
					"# Crea la base de datos si no existe\n",
					"def createDatabase(database_name):\n",
					"    return spark.sql(\"CREATE DATABASE IF NOT EXISTS {0}\".format(database_name))\n",
					"\n",
					"# Verifica si existe una carpeta\n",
					"def checkFolderExist(ls_path):\n",
					"    try:\n",
					"        mssparkutils.fs.ls(ls_path)\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        if 'java.io.FileNotFoundException' in str(e):\n",
					"            return False\n",
					"        else:\n",
					"            raise\n",
					"\n",
					"# LoadFiles\n",
					"def loadFiles(path):\n",
					"    try:\n",
					"        df = spark.read             .format('csv')             .option(\"header\", True)             .option(\"delimeter\", \",\")             .option(\"multiline\", True)             .load(path)\n",
					"        return df.withColumn(\"filename\", input_file_name())\n",
					"    except Exception as e:\n",
					"        if 'java.io.FileNotFoundException' in str(e):\n",
					"            return False\n",
					"        else:\n",
					"            raise\n",
					"\n",
					"# Borra la tabla de spark de base de datos\n",
					"def dropTable(database, table, path):\n",
					"    spark.sql(\"DROP TABLE IF EXISTS {0}\".format(f\"{database}.{table}\"))\n",
					"    if checkFolderExist(path):\n",
					"        mssparkutils.fs.rm(path, True)\n",
					"\n",
					"def loadCsv(**kwargs):\n",
					"    kwargs[\"df\"].write.option(\"header\",\"true\").option(\"delimiter\",\",\").option(\"escape\",\"\\\"\").option(\"maxRecordsPerFile\", 820000).mode(\"overwrite\").csv(kwargs[\"path\"])\n",
					"    \n",
					"def allColumns(df, metadata_columns):\n",
					"    return [column for column in df.columns if column not in metadata_columns]\n",
					"    \n",
					"def generatePKs(primaryKeys):\n",
					"    pks_dict = dict()\n",
					"    for index, value in enumerate(primaryKeys):\n",
					"        pks_dict[f\"deltaTable.{value}\"] = f\"incTable.{value}\"\n",
					"    return str(pks_dict).replace(\"{\", \"\").replace(\"}\",\"\").replace(\"'\",\"\").replace(\": \", \" <=> \").replace(\",\", \" and\")\n",
					"\n",
					"def loadInc(**kwargs):\n",
					"    print (\"inicio inc load\")\n",
					"    # We generate the primary key condition, if they are not defined it is created from the entire column.\n",
					"    if kwargs[\"primarykey\"] == None or kwargs[\"primarykey\"] == \"\":\n",
					"        columns = allColumns(kwargs[\"df\"], kwargs[\"metadata_columns\"])\n",
					"        primaryKeys = generatePKs(columns)\n",
					"    else:\n",
					"        columns = primarykey.split(\",\")\n",
					"        primaryKeys = generatePKs(kwargs[\"primarykey\"])\n",
					"    \n",
					"    # We delete the possible duplicates of the incremental df\n",
					"    df = kwargs[\"df\"].dropDuplicates(columns)\n",
					"    df = df.withColumn(\"load_date\", current_timestamp())\n",
					"    df = df.withColumn(\"type_process\", lit(kwargs[\"typeProcess\"]))\n",
					"    \n",
					"    # Check if exist table\n",
					"    if checkTableExist(kwargs[\"database\"], kwargs[\"table\"]):\n",
					"        curatedTable = DeltaTable.forPath(spark, kwargs[\"path\"])\n",
					"        curatedTable.alias(\"deltaTable\")             .merge(\n",
					"                df.alias(\"incTable\")\n",
					"                ,primaryKeys) \\\n",
					"            .whenMatchedUpdateAll() \\\n",
					"            .whenNotMatchedInsertAll() \\\n",
					"            .execute()\n",
					"    else:\n",
					"        loadFull(**kwargs)\n",
					"\n",
					"def loadFull(**kwargs):\n",
					"    print (\"inicio full load\")\n",
					"    # We generate the primary key condition, if they are not defined it is created from the entire column.\n",
					"    if kwargs[\"primarykey\"] == None or kwargs[\"primarykey\"] == \"\":\n",
					"        columns = allColumns(kwargs[\"df\"], kwargs[\"metadata_columns\"])\n",
					"        primaryKeys = generatePKs(columns)\n",
					"    else:\n",
					"        columns = primarykey.split(\",\")\n",
					"        primaryKeys = generatePKs(kwargs[\"primarykey\"])\n",
					"    \n",
					"    # We delete the possible duplicates of the incremental df\n",
					"    df = kwargs[\"df\"].dropDuplicates(columns)\n",
					"\n",
					"    database = kwargs[\"database\"]\n",
					"    table = kwargs[\"table\"]\n",
					"    df = df.withColumn(\"load_date\", current_timestamp())\n",
					"    df = df.withColumn(\"type_process\", lit(kwargs[\"typeProcess\"]))\n",
					"    dropTable(database, kwargs[\"table\"], kwargs[\"path\"])\n",
					"    if kwargs[\"partitionBy\"] == None or kwargs[\"partitionBy\"] == \"\":\n",
					"        df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", kwargs[\"path\"]).option(\"escape\",\"\\\"\").saveAsTable(f\"{database}.{table}\")\n",
					"    else:\n",
					"        df = addDatesFields(df, kwargs[\"partitionBy\"])\n",
					"        if df != False:\n",
					"            df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", kwargs[\"path\"]).option(\"escape\",\"\\\"\").partitionBy(generatePartitionField(kwargs[\"partitionBy\"])).saveAsTable(f\"{database}.{table}\")\n",
					"        else:\n",
					"            df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", kwargs[\"path\"]).option(\"escape\",\"\\\"\").partitionBy(kwargs[\"partitionBy\"]).saveAsTable(f\"{database}.{table}\")\n",
					"\n",
					"def loadAppend(**kwargs):\n",
					"    print (\"inicio append load\")\n",
					"    database = kwargs[\"database\"]\n",
					"    table = kwargs[\"table\"]\n",
					"    df = kwargs[\"df\"].withColumn(\"load_date\", current_timestamp())\n",
					"    df = df.withColumn(\"type_process\", lit(kwargs[\"typeProcess\"]))\n",
					"    if kwargs[\"partitionBy\"] == None or kwargs[\"partitionBy\"] == \"\":\n",
					"        df.write.format(\"delta\").mode(\"append\").option(\"path\", kwargs[\"path\"]).option(\"escape\",\"\\\"\").saveAsTable(f\"{database}.{table}\")\n",
					"    else:\n",
					"        df = addDatesFields(df, kwargs[\"partitionBy\"])\n",
					"        if df != False:\n",
					"            df.write.format(\"delta\").mode(\"append\").option(\"path\", kwargs[\"path\"]).option(\"escape\",\"\\\"\").partitionBy(generatePartitionField(kwargs[\"partitionBy\"])).saveAsTable(f\"{database}.{table}\")\n",
					"        else:\n",
					"            df.write.format(\"delta\").mode(\"append\").option(\"path\", kwargs[\"path\"]).option(\"escape\",\"\\\"\").partitionBy(kwargs[\"partitionBy\"]).saveAsTable(f\"{database}.{table}\")\n",
					"\n",
					"def obtenerMapaFunciones():\n",
					"    mapaFuncionesQa = {'full':loadFull,'inc':loadInc,'append':loadAppend, 'csv': loadCsv}\n",
					"    return mapaFuncionesQa"
				]
			}
		]
	}
}