{
	"name": "dpool_functions",
	"properties": {
		"description": "Auxiliary functions defined for the ingestion of registers on the dedicated pool",
		"folder": {
			"name": "includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "430ae6e3-113e-41c8-9ab1-5abdb288ef47"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Includes & references**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Function definitions**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write using Basic Auth to Internal Synapse table\r\n",
					"def insertToSynapseDedicatedPool (inputDf,server,sqlUser,sqlPass,storageKey,database,schema,table):\r\n",
					"    #Setup and trigger the read DataFrame for write to Synapse Dedicated SQL Pool.\r\n",
					"    return (inputDf.write\r\n",
					"    .option(Constants.SERVER, server)\r\n",
					"    .option(Constants.USER, sqlUser)\r\n",
					"    .option(Constants.PASSWORD, sqlPass)\r\n",
					"    # if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config\r\n",
					"    #.option(Constants.TEMP_FOLDER, \"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<some_base_path_for_temporary_staging_folders>\")\r\n",
					"    # For Basic Auth, need the storage account key for the storage account where the data will be staged\r\n",
					"    .option(Constants.STAGING_STORAGE_ACCOUNT_KEY, storageKey)\r\n",
					"    .mode(\"overwrite\")\r\n",
					"    .synapsesql(f\"{database}.{schema}.{table}\", Constants.INTERNAL))\r\n",
					"\r\n",
					"#Entity processing\r\n",
					"#Retrieve all data for first/full load\r\n",
					"def fullLoad(**kwargs):\r\n",
					"    #Get arguments\r\n",
					"    lakeDatabase = kwargs[\"lakeDatabase\"]\r\n",
					"    lakeTable = kwargs[\"lakeTable\"]\r\n",
					"\r\n",
					"    #Return results\r\n",
					"    return spark.sql(f\"\"\"SELECT * \r\n",
					"    FROM {lakeDatabase}.{lakeTable}\"\"\")\r\n",
					"\r\n",
					"#Retrieve specific data for incremental load\r\n",
					"def incLoad(**kwargs):\r\n",
					"    #Get arguments\r\n",
					"    lakeDatabase = kwargs[\"lakeDatabase\"]\r\n",
					"    lakeTable = kwargs[\"lakeTable\"]\r\n",
					"    watermarkDate = kwargs[\"watermarkDate\"]\r\n",
					"\r\n",
					"    #Return results\r\n",
					"    return spark.sql(f\"\"\"SELECT *\r\n",
					"     FROM {lakeDatabase}.{lakeTable}\r\n",
					"     WHERE load_date > '{watermarkDate}'\"\"\")\r\n",
					"\r\n",
					"#Process routing functions\r\n",
					"def genProcessMapping():\r\n",
					"    processMapping = {'01':fullLoad,'02':incLoad}\r\n",
					"    return processMapping"
				]
			}
		]
	}
}